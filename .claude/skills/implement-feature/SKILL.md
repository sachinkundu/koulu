---
name: implement-feature
description: Implement a feature from completed PRD and BDD specs
user_invocable: true
model: opus
---

# Implement Feature (Vertical Slicing)

## Purpose

Execute implementation phases from approved specifications. Each phase delivers a working end-to-end slice through all layers, ensuring tests pass and features are deployable.

**Key Principle:** This skill EXECUTES phases. Use `/create-phase-plan` first to PLAN the phases.

**If no phase plan exists:** This skill treats the feature as a single-phase implementation.

---

## Usage

`/implement-feature <context>/<feature-name>`

Example: `/implement-feature identity/profile`

Or for a specific phase:

`/implement-feature <context>/<feature-name> --phase=<number>`

Example: `/implement-feature identity/profile --phase=2`

**Recommended workflow:**
1. First run `/create-phase-plan {context}/{feature}` to generate phase plan + Phase 1 tasks
2. Then run `/implement-feature {context}/{feature} --phase=1` to implement Phase 1

---

## Input Requirements

**Must exist before starting:**
- PRD: `docs/features/{context}/{feature}-prd.md` (approved)
- TDD: `docs/features/{context}/{feature}-tdd.md` (approved)
- BDD: `tests/features/{context}/{feature}.feature` (approved)

**Auto-detected (generated by `/create-phase-plan`):**
- Phase plan: `docs/features/{context}/{feature}-implementation-phases.md`
- Granular tasks: `docs/features/{context}/{feature}-phase-{N}-tasks.md`

**Output:**
- Implementation files per phase
- Passing BDD tests per phase

---

## Workflow

### Step 1: Check Branch and Create Feature Branch

**Before starting implementation:**

1. **Check current branch:**
   ```bash
   git branch --show-current
   ```

2. **If on `main` or `master`:**
   - Create a new feature branch following the naming convention: `feature/<context>-<feature-name>`
   - Example: `feature/identity-profile`
   ```bash
   git checkout -b feature/<context>-<feature-name>
   ```

3. **If already on a feature branch:**
   - For bug fixes or iterations on the same feature, stay on the current branch
   - For a completely new feature, create a new branch

**Critical:** NEVER implement features directly on `main` or `master`. Always work on a feature branch.

### Step 2: Detect Phase Plan and Granular Tasks

**Before starting implementation, check for existing plans:**

1. **Check for phase plan:**
   - Look for `docs/features/{context}/{feature}-implementation-phases.md`
   - If found: read it to understand the phase scope, BDD scenarios, and files
   - If not found and no `--phase` flag: treat as single-phase implementation

2. **Check for granular task plan:**
   - Look for `docs/features/{context}/{feature}-phase-{N}-tasks.md`
   - If found: **announce** "Following granular plan with X tasks" and follow it task-by-task
   - If not found: generate one on-the-fly using `superpowers:writing-plans` format, scoped to this phase

3. **If following a granular task plan:**
   - Execute each task in order
   - Each task follows TDD: write failing test -> run it -> implement -> run tests -> commit
   - Report progress after each task
   - If a task is unclear, ask before proceeding

**Note:** Use `/create-phase-plan` to generate phase plans and Phase 1 granular tasks before starting implementation.

### Step 3: Implement Phase

Once ready, implement the current phase:

1. **Create files** listed in phase plan
2. **Write code** following TDD architecture
3. **Write tests** for phase's BDD scenarios:
   - **Implement** step definitions for scenarios enabled in this phase
   - **Skip** scenarios for future phases with `@pytest.mark.skip(reason="Phase X: condition")`
   - **Document** which scenarios are skipped and why
4. **Run verification:**
   - BDD tests for this phase (should show "X passed, Y skipped")
   - Unit tests
   - Linting, type checking
   - Verify all skip markers include phase numbers
5. **Verify all pass** before marking complete
   - All enabled tests pass
   - All skipped tests have phase markers
   - CI green

### Step 4: Phase Completion

**CRITICAL: Read and follow Pre-Completion Verification Protocol below BEFORE marking phase complete.**

#### Pre-Completion Verification Protocol

**MANDATORY STEPS - Execute IN ORDER:**

**Option 1: Use Helper Script (Recommended)**

Run the automated verification script:
```bash
./scripts/verify-phase-complete.sh "Phase 1"
```

This script will:
- Check infrastructure is running
- Verify test database exists
- Run pytest and verify `0 failed` (RED FLAG if > 0)
- Check for `0 warnings`
- Verify coverage >=80%
- Provide clear PASS/FAIL verdict

If script exits with code 0 (success) -> Proceed to mark phase complete
If script exits with code 1 (failure) -> **STOP, fix the issues, do NOT proceed**

**Option 2: Manual Verification**

If you prefer manual verification, follow these steps:

1. **Run pytest and READ THE OUTPUT:**
   ```bash
   pytest tests/features/ --tb=short
   ```

2. **Check for the word "failed" in output:**
   - If output shows `0 failed` -> Proceed to step 3
   - If output shows `X failed` (where X > 0) -> **STOP IMMEDIATELY, GO TO "Red Flags" SECTION BELOW**

3. **Verify EXACT output format:**
   ```
   ===================== X passed, Y skipped, 0 warnings ======================
   ```
   - `0 failed` present (explicitly check this word is "0")
   - `0 warnings` present
   - If ANY number > 0 appears after "failed", **work is NOT complete**

4. **Check coverage:**
   ```bash
   ./scripts/verify.sh
   ```
   - Coverage must be >=80%
   - If coverage <80%, **work is NOT complete**

5. **Run deployability check (if user-facing feature):**
   ```bash
   ./scripts/check-deployability.sh {feature}
   ```
   - Must show: "Feature is DEPLOYABLE" with backend + frontend
   - If check fails, **work is NOT complete** (missing frontend UI)
   - Valid exceptions: Background jobs, internal APIs (requires documentation)

6. **Self-check for rationalization trap:**
   - Ask yourself: "Am I about to say 'these failures are for Phase 2-4'?"
   - If YES -> **YOU ARE MAKING A MISTAKE, DO NOT PROCEED**
   - Failing tests must be fixed OR skipped with phase markers -- NEVER committed as failing

7. **Only if ALL checks pass, mark phase complete:**

**Mark phase complete when:**
- All files created
- **pytest shows `0 failed`** (verified in step 2-3 above)
- All phase-specific tests passing (BDD + unit)
- Future tests skipped with phase markers (not failing)
- Verification scripts pass
- Coverage >=80%
- CI green (no failing tests)
- Code reviewed (if applicable)

**Report completion with:**
- Test pass/skip counts (e.g., "8 passed, 30 skipped")
- Which scenarios are skipped and for which phase
- **EXACT pytest output showing `0 failed`**
- Coverage percentage (must be >=80%)
- CI status (must be green)

**Then ask:** "Phase X complete. Proceed to Phase X+1?"

**Example completion message:**
```
Phase 1 Complete

pytest tests/features/community/test_feed.py -v
===================== 8 passed, 30 skipped, 0 warnings =======================

Scenarios: 8 passed, 30 skipped (Phase 2: 12, Phase 3: 18)
Coverage: 81%
CI: Green
Deployable: Yes

Proceed to Phase 2 (Validation & Error Handling)?
```

**Next Phase Prompt:**

After phase completion, if more phases exist in the phase plan:

```
Phase {N} complete. Next: Phase {N+1} -- {phase name}.

Generate granular task plan for Phase {N+1}? (y/n)
```

If user says yes, generate `{feature}-phase-{N+1}-tasks.md` using writing-plans format scoped to Phase {N+1}.

#### Red Flags - STOP Immediately If You See These:

**NEVER proceed if you encounter ANY of these:**

1. **pytest output shows `X failed` where X > 0**
   - "60 passed, 10 failed" -> **STOP, work is NOT complete**
   - "358 passed, 8 failed" -> **STOP, work is NOT complete**
   - What to do: Fix the failures OR add `@pytest.mark.skip(reason="Phase X: condition")` markers

2. **You're about to rationalize failures as "okay"**
   - "These 10 failures are Phase 2-4 scenarios, so it's fine" -> **WRONG**
   - "These tests aren't relevant to this phase" -> **WRONG, skip them with markers**
   - "CI will be green after Phase 2" -> **WRONG, CI must be green NOW**
   - What to do: Stop rationalizing, fix or skip the tests

3. **You're conflating FAILING with SKIPPED**
   - "30 scenarios are for future phases" + "10 tests failing" -> **NOT the same thing**
   - SKIPPED = properly marked with `@pytest.mark.skip(reason="Phase X: ...")`
   - FAILING = missing step definitions, broken code, not implemented yet
   - What to do: Add skip markers to convert FAILING -> SKIPPED

4. **Coverage below 80%**
   - "Coverage: 53%" -> **STOP, work is NOT complete**
   - What to do: Add unit tests for untested domain logic

5. **You're about to mark work complete without running verification**
   - Skipping `pytest tests/features/` command -> **WRONG**
   - Not checking `./scripts/verify.sh` output -> **WRONG**
   - What to do: Run ALL verification commands and READ the output

6. **You see warnings and are about to ignore them**
   - "5 warnings" in pytest output -> **STOP, fix root cause**
   - What to do: Fix warnings at source, don't suppress

**If you encounter ANY red flag:**
1. **STOP** - Do not mark work complete
2. **DO NOT COMMIT** - Do not create git commits
3. **FIX OR SKIP** - Either implement missing code or add proper skip markers
4. **VERIFY AGAIN** - Re-run verification checklist
5. **ONLY THEN** proceed if all checks pass

**Remember:** User gave clear mandate: "CI must be green with 0 failures". This is non-negotiable.

### Step 5: Consider E2E Tests (Optional)

After phase completion, consider if E2E tests would add value:

**When to consider E2E tests:**
- Feature involves multi-page user journey
- Feature has complex frontend-backend integration
- Feature is critical to business (auth, payment, core workflow)
- Frontend UI is implemented (Phase 4+)

**When to skip E2E tests:**
- Backend-only implementation (BDD tests sufficient)
- Simple CRUD (unit + BDD coverage adequate)
- Internal/admin features (lower priority)
- No frontend UI yet

**Important:** E2E tests are NOT part of Definition of Done. They are recommended for critical flows but optional. Use `/e2e-test` command to add E2E tests after phase completion if needed.

**See:** `docs/testing/e2e-testing-design.md` for E2E testing strategy and best practices.

### Step 6: Manual Testing Guide

**MANDATORY: After every phase/slice completion, provide a concise manual testing guide.**

The user wants to see what was built and how the UI looks. After all verification passes, print a guide with:

1. **What was implemented** (1-3 sentence summary of user-visible changes)
2. **How to start the app** (commands to run backend + frontend if not already running)
3. **Step-by-step testing instructions** -- concrete actions the user can perform in the browser:
   - Which URL to open
   - What to click / fill in
   - What they should see as a result
4. **Expected behavior** for each action (so the user knows if it's working)

**Format:**

```markdown
## Manual Testing Guide

**What's new:** [1-3 sentence summary]

**Start the app** (if not running):
```bash
./start.sh  # or docker compose up + uvicorn + npm run dev
```

**Test it:**

1. Open http://localhost:5173/community
2. [Action] -> [Expected result]
3. [Action] -> [Expected result]
...
```

**Guidelines:**
- Keep it short -- 5-10 steps max
- Focus on user-visible functionality, not backend internals
- Include both happy path and one error case if relevant
- If feature is backend-only (no UI), show a curl command instead
- If multiple slices were implemented, cover all of them in one guide

**Example:**

```markdown
## Manual Testing Guide

**What's new:** Posts can now be pinned by admins/moderators, feed supports
Hot/New/Top sorting, and admins can create/edit/delete categories.

**Test it:**

1. Open http://localhost:5173/community
2. Log in as an admin user
3. Click the sort pills (Hot / New / Top) above the feed -> posts reorder
4. Create a post -> click "Create Post", fill in title + content, submit
5. Try creating 11 posts rapidly -> 11th should show "Rate limit exceeded"
6. Pin a post (admin menu) -> pinned post jumps to top of feed with pin icon
```

---

## Test Management Across Phases

### Handling Tests That Can't Pass Yet

**Principle:** All committed code must have CI passing. Tests for future phases should be disabled with clear phase markers.

**When to skip tests:**
- BDD scenario requires functionality planned for a future phase
- Test depends on endpoints/components not yet implemented
- Feature is partially implemented (e.g., Phase 1 does create, Phase 2 does update)

**How to skip tests:**

#### Python (pytest-bdd)

```python
import pytest

# Mark individual scenario
@pytest.mark.skip(reason="Phase 2: Will be enabled when validation is implemented")
@scenario('features/posts.feature', 'Invalid post title rejected')
def test_invalid_post_rejected():
    pass

# Mark entire test function
@pytest.mark.skip(reason="Phase 3: Will be enabled when GET /posts endpoint is implemented")
def test_user_views_posts():
    """Scenario: User views list of posts"""
    pass

# Conditional skip based on phase
@pytest.mark.skipif(
    not hasattr(sys.modules['app.api.posts'], 'update_post'),
    reason="Phase 3: Waiting for update_post endpoint"
)
def test_user_updates_post():
    pass
```

#### Frontend (Vitest)

```typescript
import { describe, it, expect } from 'vitest';

// Skip individual test
it.skip('should show validation error for empty title', () => {
  // Phase 2: Will be enabled when validation UI is implemented
  expect(true).toBe(true);
});

// Skip entire test suite
describe.skip('Post editing', () => {
  // Phase 3: Will be enabled when PostEditForm is implemented
  it('should update post title', () => {
    // ...
  });
});
```

### Test Skip Documentation

**Always include in skip reason:**
1. **Phase number:** When test will be enabled
2. **Condition:** What needs to be implemented
3. **Optional context:** Why it's deferred

**Good skip reasons:**
```python
# Clear phase and condition
@pytest.mark.skip(reason="Phase 2: Will be enabled when DisplayName value object is implemented")

# Specific feature dependency
@pytest.mark.skip(reason="Phase 3: Requires POST /posts/:id/comments endpoint (Phase 3)")

# Technical blocker with phase
@pytest.mark.skip(reason="Phase 4: Needs frontend PostEditForm component")
```

**Bad skip reasons:**
```python
# No phase information
@pytest.mark.skip(reason="Not implemented yet")

# Too vague
@pytest.mark.skip(reason="TODO")

# No clear enablement condition
@pytest.mark.skip(reason="Later")
```

### Phase Completion Checklist with Skipped Tests

**Before marking phase complete:**

1. **All phase-specific tests pass:**
   ```bash
   pytest tests/features/posts.py -v
   # Should show: X passed, Y skipped (with Phase markers)
   ```

2. **Verify skip markers are correct:**
   ```bash
   # List all skipped tests with reasons
   pytest tests/features/ -v | grep "SKIPPED"

   # Verify all skips reference future phases
   grep -r "@pytest.mark.skip" tests/features/ | grep -v "Phase [2-9]"
   # Should return nothing (all skips have phase markers)
   ```

3. **Document skipped tests in phase summary:**
   ```markdown
   ## Phase 1 Complete

   **Tests Passing:** 8/8 phase-specific scenarios
   **Tests Skipped:** 12 scenarios (Phase 2: 7, Phase 3: 5)
   **CI Status:** Green

   **Skipped Scenarios (will be enabled in Phase 2):**
   - Invalid post title rejected (validation)
   - Empty post content rejected (validation)
   - Duplicate post prevented (validation)
   ...
   ```

4. **Update phase plan with actual skip count:**
   ```markdown
   ## Phase 1: Core Happy Path - COMPLETE
   - Tests Passing: 8/8
   - Tests Skipped: 12 (documented for Phase 2+)
   - CI: Green
   ```

### Moving from Skipped to Active

**When starting Phase 2:**

1. **List tests to enable:**
   ```bash
   # Find all tests marked for Phase 2
   grep -r "Phase 2:" tests/features/ | grep "@pytest.mark.skip"
   ```

2. **Remove skip markers:**
   ```python
   # Before (Phase 1)
   @pytest.mark.skip(reason="Phase 2: Will be enabled when validation is implemented")
   @scenario('features/posts.feature', 'Invalid post title rejected')
   def test_invalid_post_rejected():
       pass

   # After (Phase 2)
   @scenario('features/posts.feature', 'Invalid post title rejected')
   def test_invalid_post_rejected():
       pass
   ```

3. **Verify newly enabled tests pass:**
   ```bash
   # Run only Phase 2 tests (remove skip markers first)
   pytest tests/features/posts.py::test_invalid_post_rejected -v
   # Must pass before marking Phase 2 complete
   ```

### Alternative: Feature Flags (Advanced)

For features that need gradual rollout:

```python
# Environment-based skipping
@pytest.mark.skipif(
    os.getenv('FEATURE_POST_EDITING') != 'true',
    reason="Feature flag: POST_EDITING not enabled"
)
def test_user_edits_post():
    pass
```

**When to use:**
- Feature needs A/B testing
- Gradual rollout to production
- Feature toggle in codebase

**When NOT to use:**
- Simple phased implementation (use phase markers instead)
- Tests will definitely be enabled in next phase

---

## Quality Checklist

Before marking any phase complete:

**Required (Definition of Done):**
- [ ] All phase files created
- [ ] Code follows existing patterns
- [ ] All phase-specific BDD scenarios passing (X passed)
- [ ] Future BDD scenarios skipped with phase markers (Y skipped, documented)
- [ ] Unit tests written and passing
- [ ] No regressions (existing tests still pass)
- [ ] Linting passes (ruff)
- [ ] Type checking passes (mypy)
- [ ] No security issues introduced
- [ ] Documentation updated (if needed)
- [ ] Git commit created with clear message
- [ ] **CI green** (all enabled tests pass, skipped tests have phase markers)

**Test Skip Verification:**
- [ ] Run `pytest tests/features/{context}/ -v` shows "X passed, Y skipped"
- [ ] Run `grep -r "@pytest.mark.skip" tests/features/{context}/` shows phase markers
- [ ] All skip reasons include phase number and condition
- [ ] Skipped tests documented in phase completion summary

**Optional (Recommended for critical flows):**
- [ ] E2E tests added for critical user journeys (use `/e2e-test` if needed)
